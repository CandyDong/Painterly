{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "from DRL.evaluator import Evaluator\n",
    "from utils.util import *\n",
    "from utils.tensorboard import TensorBoard\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = os.path.abspath('.').split('/')[-1]\n",
    "writer = TensorBoard('../train_log/{}'.format(exp))\n",
    "os.system('ln -sf ../train_log/{} ./log'.format(exp))\n",
    "# os.system('mkdir ./model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, evaluate):\n",
    "    train_times = args.train_times\n",
    "    env_batch = args.env_batch\n",
    "    validate_interval = args.validate_interval\n",
    "    max_step = args.max_step\n",
    "    debug = args.debug\n",
    "    episode_train_times = args.episode_train_times\n",
    "    resume = args.resume\n",
    "    output = args.output\n",
    "    time_stamp = time.time()\n",
    "    step = episode = episode_steps = 0\n",
    "    tot_reward = 0.\n",
    "    observation = None\n",
    "    noise_factor = args.noise_factor\n",
    "    while step <= train_times:\n",
    "        step += 1\n",
    "        episode_steps += 1\n",
    "        # reset if it is the start of episode\n",
    "        if observation is None:\n",
    "            observation = env.reset()\n",
    "            agent.reset(observation, noise_factor)    \n",
    "        action = agent.select_action(observation, noise_factor=noise_factor)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        agent.observe(reward, observation, done, step)\n",
    "        if (episode_steps >= max_step and max_step):\n",
    "            if step > args.warmup:\n",
    "                # [optional] evaluate\n",
    "                if episode > 0 and validate_interval > 0 and episode % validate_interval == 0:\n",
    "                    reward, dist = evaluate(env, agent.select_action, debug=debug)\n",
    "                    if debug: prRed('Step_{:07d}: mean_reward:{:.3f} mean_dist:{:.3f} var_dist:{:.3f}'.format(step - 1, np.mean(reward), np.mean(dist), np.var(dist)))\n",
    "                    writer.add_scalar('validate/mean_reward', np.mean(reward), step)\n",
    "                    writer.add_scalar('validate/mean_dist', np.mean(dist), step)\n",
    "                    writer.add_scalar('validate/var_dist', np.var(dist), step)\n",
    "                    agent.save_model(output)\n",
    "            train_time_interval = time.time() - time_stamp\n",
    "            time_stamp = time.time()\n",
    "            tot_Q = 0.\n",
    "            tot_value_loss = 0.\n",
    "            if step > args.warmup:\n",
    "#                 if step < 10000 * max_step:\n",
    "#                     lr = (3e-4, 1e-3)\n",
    "#                 elif step < 20000 * max_step:\n",
    "#                     lr = (1e-4, 3e-4)\n",
    "#                 else:\n",
    "#                     lr = (3e-5, 1e-5)\n",
    "                if step < 1000 * max_step:\n",
    "                    lr = (3e-4, 1e-3)\n",
    "                elif step < 2000 * max_step:\n",
    "                    lr = (1e-4, 3e-4)\n",
    "                else:\n",
    "                    lr = (3e-5, 1e-5)\n",
    "                for i in range(episode_train_times):\n",
    "                    Q, value_loss = agent.update_policy(lr)\n",
    "                    tot_Q += Q.data.cpu().numpy()\n",
    "                    tot_value_loss += value_loss.data.cpu().numpy()\n",
    "                writer.add_scalar('train/critic_lr', lr[0], step)\n",
    "                writer.add_scalar('train/actor_lr', lr[1], step)\n",
    "                writer.add_scalar('train/Q', tot_Q / episode_train_times, step)\n",
    "                writer.add_scalar('train/critic_loss', tot_value_loss / episode_train_times, step)\n",
    "            if debug: prBlack('#{}: steps:{} interval_time:{:.2f} train_time:{:.2f}' \\\n",
    "                .format(episode, step, train_time_interval, time.time()-time_stamp)) \n",
    "            time_stamp = time.time()\n",
    "            # reset\n",
    "            observation = None\n",
    "            episode_steps = 0\n",
    "            episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "class Arg():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 96\n",
    "        self.max_step = 40\n",
    "        self.warmup = 400\n",
    "        self.discount = 0.95**5\n",
    "        self.rmsize = 800\n",
    "        self.env_batch = 96\n",
    "        self.tau = 0.001\n",
    "        self.noise_factor = 0;\n",
    "        self.validate_interval = 50\n",
    "        self.validate_episodes = 5\n",
    "        self.train_times = 196000\n",
    "        self.episode_train_times = 10\n",
    "        self.resume = None\n",
    "        self.debug = True\n",
    "        self.output = './model'\n",
    "        self.seed = 1234\n",
    "\n",
    "args = Arg()\n",
    "# parser = argparse.ArgumentParser(description='Learning to Paint')\n",
    "\n",
    "# # hyper-parameter\n",
    "# parser.add_argument('--warmup', default=400, type=int, help='timestep without training but only filling the replay memory')\n",
    "# parser.add_argument('--discount', default=0.95**5, type=float, help='discount factor')\n",
    "# parser.add_argument('--batch_size', default=96, type=int, help='minibatch size')\n",
    "# parser.add_argument('--rmsize', default=800, type=int, help='replay memory size')\n",
    "# parser.add_argument('--env_batch', default=96, type=int, help='concurrent environment number')\n",
    "# parser.add_argument('--tau', default=0.001, type=float, help='moving average for target network')\n",
    "# parser.add_argument('--max_step', default=40, type=int, help='max length for episode')\n",
    "# parser.add_argument('--noise_factor', default=0, type=float, help='noise level for parameter space noise')\n",
    "# parser.add_argument('--validate_interval', default=50, type=int, help='how many episodes to perform a validation')\n",
    "# parser.add_argument('--validate_episodes', default=5, type=int, help='how many episode to perform during validation')\n",
    "# parser.add_argument('--train_times', default=2000000, type=int, help='total traintimes')\n",
    "# parser.add_argument('--episode_train_times', default=10, type=int, help='train times for each episode')    \n",
    "# parser.add_argument('--resume', default=None, type=str, help='Resuming model path for testing')\n",
    "# parser.add_argument('--output', default='./model', type=str, help='Resuming model path for testing')\n",
    "# parser.add_argument('--debug', dest='debug', action='store_true', help='print some info')\n",
    "# parser.add_argument('--seed', default=1234, type=int, help='random seed')\n",
    "\n",
    "# args = parser.parse_args()    \n",
    "# args.output = get_output_folder(args.output, \"Paint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 10000 images\n",
      "loaded 20000 images\n",
      "loaded 30000 images\n",
      "loaded 40000 images\n",
      "loaded 50000 images\n",
      "loaded 60000 images\n",
      "loaded 70000 images\n",
      "loaded 80000 images\n",
      "loaded 90000 images\n",
      "loaded 100000 images\n",
      "loaded 110000 images\n",
      "loaded 120000 images\n",
      "loaded 130000 images\n",
      "loaded 140000 images\n",
      "loaded 150000 images\n",
      "loaded 160000 images\n",
      "loaded 170000 images\n",
      "loaded 180000 images\n",
      "loaded 190000 images\n",
      "loaded 200000 images\n",
      "finish loading data, 197999 training images, 2001 testing images\n",
      "observation_space (96, 128, 128, 7) action_space 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/LearningToPaint/baseline/DRL/ddpg.py:161: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s0 = torch.tensor(self.state, device='cpu')\n",
      "/home/ubuntu/LearningToPaint/baseline/DRL/ddpg.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s1 = torch.tensor(state, device='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[98m #0: steps:40 interval_time:6.17 train_time:0.00\u001b[00m\n",
      "\u001b[98m #1: steps:80 interval_time:4.52 train_time:0.00\u001b[00m\n",
      "\u001b[98m #2: steps:120 interval_time:4.41 train_time:0.00\u001b[00m\n",
      "\u001b[98m #3: steps:160 interval_time:4.40 train_time:0.00\u001b[00m\n",
      "\u001b[98m #4: steps:200 interval_time:4.53 train_time:0.00\u001b[00m\n",
      "\u001b[98m #5: steps:240 interval_time:4.53 train_time:0.00\u001b[00m\n",
      "\u001b[98m #6: steps:280 interval_time:4.40 train_time:0.00\u001b[00m\n",
      "\u001b[98m #7: steps:320 interval_time:4.40 train_time:0.00\u001b[00m\n",
      "\u001b[98m #8: steps:360 interval_time:4.26 train_time:0.00\u001b[00m\n",
      "\u001b[98m #9: steps:400 interval_time:4.16 train_time:0.00\u001b[00m\n",
      "\u001b[98m #10: steps:440 interval_time:4.15 train_time:21.76\u001b[00m\n",
      "\u001b[98m #11: steps:480 interval_time:4.16 train_time:18.84\u001b[00m\n",
      "\u001b[98m #12: steps:520 interval_time:4.15 train_time:18.87\u001b[00m\n",
      "\u001b[98m #13: steps:560 interval_time:4.15 train_time:18.88\u001b[00m\n",
      "\u001b[98m #14: steps:600 interval_time:4.18 train_time:18.89\u001b[00m\n",
      "\u001b[98m #15: steps:640 interval_time:4.17 train_time:18.89\u001b[00m\n",
      "\u001b[98m #16: steps:680 interval_time:4.17 train_time:18.91\u001b[00m\n",
      "\u001b[98m #17: steps:720 interval_time:4.16 train_time:18.93\u001b[00m\n",
      "\u001b[98m #18: steps:760 interval_time:4.16 train_time:18.92\u001b[00m\n",
      "\u001b[98m #19: steps:800 interval_time:4.19 train_time:18.93\u001b[00m\n",
      "\u001b[98m #20: steps:840 interval_time:4.18 train_time:18.95\u001b[00m\n",
      "\u001b[98m #21: steps:880 interval_time:4.20 train_time:18.96\u001b[00m\n",
      "\u001b[98m #22: steps:920 interval_time:4.18 train_time:18.95\u001b[00m\n",
      "\u001b[98m #23: steps:960 interval_time:4.18 train_time:18.97\u001b[00m\n",
      "\u001b[98m #24: steps:1000 interval_time:4.18 train_time:18.98\u001b[00m\n",
      "\u001b[98m #25: steps:1040 interval_time:4.19 train_time:18.99\u001b[00m\n",
      "\u001b[98m #26: steps:1080 interval_time:4.18 train_time:18.99\u001b[00m\n",
      "\u001b[98m #27: steps:1120 interval_time:4.19 train_time:18.99\u001b[00m\n",
      "\u001b[98m #28: steps:1160 interval_time:4.17 train_time:18.97\u001b[00m\n",
      "\u001b[98m #29: steps:1200 interval_time:4.20 train_time:18.98\u001b[00m\n",
      "\u001b[98m #30: steps:1240 interval_time:4.22 train_time:18.98\u001b[00m\n",
      "\u001b[98m #31: steps:1280 interval_time:4.27 train_time:18.99\u001b[00m\n",
      "\u001b[98m #32: steps:1320 interval_time:4.20 train_time:18.98\u001b[00m\n",
      "\u001b[98m #33: steps:1360 interval_time:4.22 train_time:18.99\u001b[00m\n",
      "\u001b[98m #34: steps:1400 interval_time:4.19 train_time:18.98\u001b[00m\n",
      "\u001b[98m #35: steps:1440 interval_time:4.18 train_time:18.99\u001b[00m\n",
      "\u001b[98m #36: steps:1480 interval_time:4.18 train_time:18.99\u001b[00m\n",
      "\u001b[98m #37: steps:1520 interval_time:4.19 train_time:18.97\u001b[00m\n",
      "\u001b[98m #38: steps:1560 interval_time:4.19 train_time:18.98\u001b[00m\n",
      "\u001b[98m #39: steps:1600 interval_time:4.22 train_time:18.99\u001b[00m\n",
      "\u001b[98m #40: steps:1640 interval_time:4.31 train_time:18.99\u001b[00m\n",
      "\u001b[98m #41: steps:1680 interval_time:4.31 train_time:18.97\u001b[00m\n",
      "\u001b[98m #42: steps:1720 interval_time:4.26 train_time:18.98\u001b[00m\n",
      "\u001b[98m #43: steps:1760 interval_time:4.19 train_time:18.98\u001b[00m\n",
      "\u001b[98m #44: steps:1800 interval_time:4.21 train_time:18.97\u001b[00m\n",
      "\u001b[98m #45: steps:1840 interval_time:4.20 train_time:18.96\u001b[00m\n",
      "\u001b[98m #46: steps:1880 interval_time:4.27 train_time:18.96\u001b[00m\n",
      "\u001b[98m #47: steps:1920 interval_time:4.18 train_time:18.96\u001b[00m\n",
      "\u001b[98m #48: steps:1960 interval_time:4.17 train_time:18.97\u001b[00m\n",
      "\u001b[98m #49: steps:2000 interval_time:4.30 train_time:18.95\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "# if torch.cuda.is_available(): torch.cuda.manual_seed_all(args.seed)\n",
    "# random.seed(args.seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from DRL.ddpg import DDPG\n",
    "from DRL.multi import fastenv\n",
    "# fenv = fastenv(args.max_step, args.env_batch, writer)\n",
    "# agent = DDPG(args.batch_size, args.env_batch, args.max_step, \\\n",
    "#              args.tau, args.discount, args.rmsize, \\\n",
    "#              writer, args.resume, args.output)\n",
    "# evaluate = Evaluator(args, writer)\n",
    "fenv = fastenv(args.max_step, args.env_batch, writer)\n",
    "agent = DDPG(args.batch_size, args.env_batch, args.max_step, \\\n",
    "             args.tau, args.discount, args.rmsize, \\\n",
    "             writer, args.resume, args.output)\n",
    "evaluate = Evaluator(args, writer)\n",
    "print('observation_space', fenv.observation_space, 'action_space', fenv.action_space)\n",
    "train(agent, fenv, evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
